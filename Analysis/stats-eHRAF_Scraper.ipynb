{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats for eHRAF Scraper\n",
    "The current stats are merely to \n",
    "- Clean and reorganize the dataframe \n",
    "- Find the most common OCM codes\n",
    "- Find association rules (when one OCM appears this other OCM is likely to appear)\n",
    "More work will be done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 # dataframe storing\n",
    "import numpy as np\n",
    "import re                           # regex for searching through strings\n",
    "\n",
    "# put folder name here\n",
    "folder = r'subjects-(sickness)_FILTERS-culture_level_samples(PSF)'\n",
    "\n",
    "\n",
    "df = pd.read_excel('../Data/' + folder + '/_Altogether_Dataset.xlsx')\n",
    "# Turn the string of column OCM back into a list \n",
    "df['OCM'] = df.OCM.apply(lambda x: re.sub(\" \",'',x))\n",
    "df['OCM'] = df.OCM.apply(lambda x: x[1:-1].split(','))\n",
    "\n",
    "# did it work? did it output a single OCM string?\n",
    "df['OCM'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop all rows that have a blank passage\n",
    "print(f'Before: {len(df)}')\n",
    "df = df.dropna(subset=\"Passage\")\n",
    "print(f'After: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use a higher order code (750) eHRAF attempts to aquire ALL OCMs related to your input.\n",
    "# select only the OCMs we originally wished to search for by inputting OCM's into a list\n",
    "lst = [\"750\",\"751\",\"752\",\"753\"]\n",
    "# lst = [\"751\", \"752\"]\n",
    "msk = df['OCM'].apply(lambda x: not set(x).isdisjoint(lst))\n",
    "df = df.loc[msk]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates\n",
    "Currently, only passages will be removed if they contain a duplicate passage with the same OCMs. Duplicate passages with different OCMs will remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (exploratory)  \n",
    "# Find all passages which are duplicates but do not share the same document. \n",
    "# First let's explore some of the duplicates\n",
    "dup1 = df[\"Passage\"].duplicated(keep=False)  # find all duplicate passages\n",
    "dup2 = df[dup1].duplicated(subset=[\"Passage\", \"DocTitle\"], keep=False) #of the duplicate passages, find those that shair a passage and doc title\n",
    "# rows which contain duplicate passages but not part of the same document (only top 4 shown)\n",
    "print(f'Number of passages whose duplicates come from different documents: {len(df[dup1][~dup2].sort_values(by=\"Passage\"))}')\n",
    "df[dup1][~dup2].sort_values(by='Passage').head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (exploratory) \n",
    "# Find passages which have duplicates but whose duplicates have different OCM numbers\n",
    "df = df.copy()\n",
    "df[\"OCM\"] = df['OCM'].apply(tuple) #turn the OCM list to a tuple to allow for comparisons\n",
    "\n",
    "# Of the passages which have duplicates, find and keep all which have the same OCM\n",
    "dup3 = df[dup1].duplicated(subset=[\"Passage\", \"OCM\"], keep=False)\n",
    "# Show only the passages with duplicates but NOT matching OCMs\n",
    "print(f'Number of passages whose duplicates do not share OCMs:  {len(df[dup1][~dup3].sort_values(by=\"Passage\"))}')\n",
    "df[\"OCM\"] = df['OCM'].apply(list)\n",
    "df[dup1][~dup3].sort_values(by=\"Passage\").head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all duplicated passages which share OCMs\n",
    "\n",
    "df[\"OCM\"] = df['OCM'].apply(tuple) #turn the OCM list to a tuple to allow for comparisons\n",
    "\n",
    "# drop duplicates\n",
    "print(f'Before {len(df)}')\n",
    "df.drop_duplicates(subset=[\"Passage\", \"OCM\"], keep='first', inplace=True)\n",
    "print(f'After {len(df)}')\n",
    "\n",
    "df[\"OCM\"] = df['OCM'].apply(list) #turn the OCM back to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shave OCMs\n",
    "And make an exploded OCM dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataset in which each OCM have its own row by exploding (you can reset the index with .reset_index(drop=True))\n",
    "df_OCM = df.explode(column='OCM').reset_index(drop=True)\n",
    "# Find OCM's that do not fit the normal 100-900 OCM scheme\n",
    "# NOTE 0 means the material is not relevant, I am unsure, however, why this sometimes appears with other OCM's in the same passage\n",
    "# NOTE I believe 5310 and 5311 are different specifications of 531 while 1710 might be a more specific (and singlular) subset of 171? I do not believe the same for 77 and 1787\n",
    "list_OCM = df_OCM['OCM'].value_counts().index.tolist()\n",
    "small_OCM = [x for x in list_OCM if len(x) <3 or len(x) > 3]\n",
    "small_OCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove and shave OCM codes\n",
    "# add to the list for codes which should be removed\n",
    "remove_list = ['1787','77']\n",
    "print(f'starting list {len(df_OCM)}')\n",
    "for i in remove_list:\n",
    "    df_OCM = df_OCM[df_OCM[\"OCM\"] != i]\n",
    "# \"Shave\" the OCM codes that seem to have a parent (5310 and 5311 become 531).\n",
    "df_OCM['OCM'] = df_OCM.OCM.apply(lambda x: x[0:3] if len(x) >= 3 else x)\n",
    "print(f'Ending list {len(df_OCM)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the removals like above to the original dataframe (this is easier than just imploding as there are duplicates which limit this)\n",
    "\n",
    "# remove specified OCM codes\n",
    "df[\"OCM\"] = df[\"OCM\"].apply(lambda x: [item for item in x if item not in remove_list])\n",
    "# shorten the 'small_OCM' OCMs so that 5310 becomes 531\n",
    "df[\"OCM\"] = df[\"OCM\"].apply(lambda x: [item[0:3] if item in small_OCM else item for item in x])\n",
    "print(len(df))\n",
    "# explantaion of above list comprehension: go through every row of the column \"OCM\" (via apply) \n",
    "# lambda x is an anonymous function which takes the row \"x\" and inputs it into the function.\n",
    "# each row has its list items iterated over ( \"___ for item in x\") and checked if each list item is part of the small_OCM list, if so,\n",
    "# return the first 3 characters, if not, return the original list item. Return everything back as a list and apply it to the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary for later count comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of passages for each culture\n",
    "culture_set = set(df[\"Culture\"])\n",
    "culture_dict = {}\n",
    "it_count = 0\n",
    "for cult_i in culture_set:\n",
    "    row_count = len(df.loc[df[\"Culture\"]==cult_i])\n",
    "    culture_dict[cult_i] = row_count\n",
    "    it_count += row_count\n",
    "print(f'Passages: {it_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# Quick search for OCMs regardless of culture\n",
    "# NOTE, sometimes a higher order code like 750 appears without lower order codes)\n",
    "lst = [\"159\"] #enter your OCM strings here separated by a comma\n",
    "msk = df['OCM'].apply(lambda x: not set(x).isdisjoint(lst))\n",
    "out = df.loc[msk]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick search for OCMs SUBINDEX BY ANOTHER COLUMN\n",
    "lst = [\"159\"] #enter your OCM strings here separated by a comma\n",
    "msk = df.loc[df[\"Culture\"]== \"Akan\"]['OCM'].apply(lambda x: not set(x).isdisjoint(lst))\n",
    "out = df.loc[msk.index][msk]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL)\n",
    "# There are some passages that describe previous passages but do not contain information themselves like: \n",
    "# \"Notes\" or \"End\" or \"Log\"\n",
    "# This code cell indicates (but does not remove) how many passages are short like the ones described which \n",
    "# may disrupt our OCM stats because they contain OCMs without actually having text that refers to these OCMs\n",
    "shortPass_list = []\n",
    "for i in df['Passage']:\n",
    "    if len(i)<=10:\n",
    "        shortPass_list.append(i)\n",
    "print(f'Number of passages with text with 10 of fewer characters: {len(shortPass_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "OCM"
    ]
   },
   "source": [
    "## OCM Code Counting\n",
    "Count every OCM within each culture. Do not count OCM's specified by the search (like if searched for 750-755, do not count these). \n",
    "<!-- - REMOVE all passages which are blank since we can't very well do lexical searches on them -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of df_OCM as to not interfere with other analysis\n",
    "df_OCM_freq = df_OCM.copy()\n",
    "# Then turn the OCM's back to an integer (for removals)\n",
    "df_OCM_freq['OCM'] = df_OCM_freq.OCM.apply(lambda x: int(x))\n",
    "# only keep OCMs outside our search parameters whatever those are\n",
    "df_sub_ex = df_OCM_freq.loc[(df_OCM_freq[\"OCM\"]<750) | (df_OCM_freq[\"OCM\"]>754)]\n",
    "\n",
    "# Overwrite and create a new dataframe for OCM counts and frequencies\n",
    "df_OCM_freq = pd.DataFrame(columns=[\"Culture\",\"OCM\",\"Frequency\",\"Proportion_of_Passages\"])\n",
    "for key, val in culture_dict.items():\n",
    "    value_count = df_sub_ex.loc[df_sub_ex[\"Culture\"]==key][\"OCM\"].value_counts()\n",
    "    # duplicate the culture word and asign it to each of its rows\n",
    "    cult_count = [key] * len(value_count)\n",
    "    # create a culture dataframe and append it to to the \n",
    "    df_OCM_Concat = pd.DataFrame({\"Culture\":cult_count,\"OCM\":value_count.index, \"Frequency\":value_count.values, \"Proportion_of_Passages\":value_count.values/val})\n",
    "    df_OCM_freq = pd.concat([df_OCM_freq, df_OCM_Concat], ignore_index=True)\n",
    "df_OCM_freq = df_OCM_freq.sort_values(by = [\"Culture\", \"Frequency\"], ascending= [True, False])\n",
    "df_OCM_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'OCMs per culture: {sum(df_OCM_freq[\"Frequency\"]) / len(set(df_OCM_freq[\"Culture\"]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file\n",
    "df_OCM_freq.to_csv(\"Culture_Frequency.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules for OCMs\n",
    "Using Machine Learning, we will attempt to determine the co-occurance of OCMs. For example, if the OCM code 262 is present, what is the likelihood that both 751 and 752 would be present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resources\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# We will use the apriori module to generate a dataframe that\n",
    "# we can use for association rule finding\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# We will use the association_rules module to generate\n",
    "# our association rules from the apriori output data frame\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display important columns\n",
    "df_smaller = df_OCM[['Culture', 'OCM','Passage']]\n",
    "df_smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a grouped dataframe object by Culture and Passage \n",
    "df_group = df_smaller.groupby(by = ['Culture', 'Passage'])\n",
    "df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_OCM_list(x):\n",
    "\n",
    "    '''\n",
    "    Will return a list of the unique items\n",
    "    in a particular grouping when used with\n",
    "    the agg method as its function\n",
    "    '''\n",
    "\n",
    "    return x.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the agg method and make_OCM_list\n",
    "# to return a list of unique items for each ocm\n",
    "# Note that depending on the filtering, there may be duplicate passages with different OCMs which are aggregated, \n",
    "# this method will combine them and extract the unique OCMs so it may not be a problem.\n",
    "df_unique = df_group.agg(make_OCM_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_trans = list(df_unique['OCM'])\n",
    "list_trans = list_trans[0:]\n",
    "len(list_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "encoded_itemset = te.fit(list_trans).transform(list_trans)\n",
    "print(encoded_itemset.shape) # show possible transcations and number of items\n",
    "te.columns_\n",
    "\n",
    "\n",
    "\n",
    "df_encoded = pd.DataFrame(encoded_itemset, columns = te.columns_)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we begin, let's do a small\n",
    "# amount of cleanup.  Let's remove all\n",
    "# columns (items) that have less than 1 characters since that is just blank space\n",
    "# more data cleaning my be required as time continues in case errors become evident in the scraped dataset\n",
    "OCM_items = list(filter(lambda x: len(x) < 1, te.columns_ ))\n",
    "print(\"removed: \",  OCM_items)\n",
    "df_encoded = df_encoded.drop(columns=OCM_items) #remove small strings as they seem not to be items\n",
    "print('How many unique items are left?', len(df_encoded.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use apriori to create a dataframe with columns of support and itemset lists\n",
    "# Note that if your items are large compared to your sample (you have few rows but many columns) I reccommend using \n",
    "# a higher min_support as many more combinations may have spuriously higher support. Also, you can crash the program if too many are selected\n",
    "df_support = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
    "df_support.sort_values('support', inplace=True, ascending = False)\n",
    "df_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "OCM"
    ]
   },
   "source": [
    "## Use association_rules to find the rules\n",
    "\n",
    "Using the dataframe generated by `apriori`, find the association rules with the greatest lift.  See the [association_rules documentation](https://rasbt.github.io/mlxtend/api_modules/mlxtend.frequent_patterns/association_rules/) for how to do this.\n",
    "\n",
    "Sort the resulting DataFrame by lift in descending order.  A lift > 1 indicates that the items are often purchased together and that buying X will increase the purchase of Y.  A lift of < 1 indicates the items are often substituted.  That is X is substituted for Y so X and Y don't appear together often.\n",
    "\n",
    "Examine the resulting DataFrame.  For the association rule X -> Y, X is the column `antecedents` and Y is the column `consequents`.  If sorted you can see the metrics for each rule based upon the lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the association rules\n",
    "rules = association_rules(df_support, metric = 'lift', min_threshold=1.0)\n",
    "# lift >1 more likely than chance X means you see Y\n",
    "# lift = 1 as often as chance\n",
    "# lift <1 (substitution) less likely than chance X means you see Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the rules by lift\n",
    "# and examine the output\n",
    "# to find what rules were\n",
    "# discovered\n",
    "rules.sort_values('lift', ascending=False, inplace =True)\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for OCM codes within the list\n",
    "lst = frozenset([\"793\",\"226\"])\n",
    "msk = rules['antecedents'].apply(lambda x: not set(x).isdisjoint(lst))\n",
    "out = rules.loc[msk]\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3394e5ff5e37174aec0305e0ed7ec30b336f01a56a90646f493ecbcd8deec75b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
