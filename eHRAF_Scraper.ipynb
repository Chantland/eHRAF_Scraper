{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraper\n",
    "\n",
    "#### **Author** -- Eric Chantland (ericchantland@gmail.com)\n",
    "#### **Created** -- October 2022\n",
    "\n",
    "\n",
    "<p>The objective of this program is to webscrap the eHRAF database for various text passages in order to explore misery and how it relates with folk/wild traditions. To run this program, merely press play and enter in the URL and your name. To obtain the URL, progress through ehRAF website and enter in your search terms in the \"advanced search\" boxes and the filters. When you have successfully filtered the searches (so the region list) but BEFORE you actually look at any of the culture's links, copy the URL that it gives you into this program. This program seeks to scrape the webpage for texts and OCM's. It is therefore very susceptible to the eHRAF webpage changing, loading slow/fast, and/or not providing standardized information. If this program fails, it is likely due to one of these three.<p>\n",
    "\n",
    "<p> To run this program, you can go individually or just run all the cells at once. However, this autonomous webpage (chrome) must be running for the rest of the webscraper to work. Therefore, if you run into an issue, just rerurn the whole program. If that does not fix it, then there must be a different issue. <p>\n",
    "\n",
    "<p> If the program stalls for whatever reason, then webpage may have loaded too slow and the scraper was not able to catch that this happened resulting in an infinite loop. The program needs to be updated for its sleep timer. <b>Stop the program and contact me.<b> <p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 # dataframe storing\n",
    "from bs4 import BeautifulSoup       # parsing web content in a nice way\n",
    "import ssl                          # MAY BE UNNECESSARY: provides access to the security socket layer (ssl) https://docs.python.org/3/library/ssl.html\n",
    "import urllib                       # MAY BE UNNECESSARY: open and navigate URL's\n",
    "import os                           # Find where this file is located.\n",
    "import time                         # for pausing the program in order for it to load the webpage\n",
    "import re                           # regex for searching through strings\n",
    "\n",
    "\n",
    "from selenium import webdriver      # load and run the webpage dynamically.\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# for wait times\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these search variables, input the text \"any\", \"none\", or \"all\" before a list of the desired keywords. This is excluding \"culture\" which only contains \"any\" and \"none\"\n",
    "\n",
    "For each of the query variables, just input a list as they are always \"Any\"\n",
    "\n",
    "Alternatively, you can copy and paste the hyperlink and get the same results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL\n",
    "On eHRAF homepage, you may put in various search terms and queries using their GUI (their menu) once you are happy with the saerch terms, Copy and paste the URL into the parenthesis of the Variable below:\n",
    "For example, to get the search for all documents containing \"apple\" and through the PSF, I got a hyperlink like this:\n",
    "https://ehrafworldcultures.yale.edu/search?q=text%3AApple&fq=culture_level_samples%7CPSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the hyperlink goes within quotes!\n",
    "URL = input(\"Please enter in an eHRAF URL, otherwise enter nothing for a demo\\n\")\n",
    "user = input(\"What is your name?\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"Hyperlink-Example.png\"> -->\n",
    "![alt text](Hyperlink-Example.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will scrape up to 382 documents and take roughly \n",
      "1 minute(s), and 28 second(s)\n"
     ]
    }
   ],
   "source": [
    "# Use a autonomous Chrome page to dynamically load the page for scraping. \n",
    "# Requires webdriver to be downloaded and then its path directed to.\n",
    "\n",
    "# iniate \"headless\" which stops chrome from showing itself when this is run, \n",
    "# switch headless to False if you want to see the webpage or True if you want it to run in the background\n",
    "options = Options()\n",
    "options.headless = False\n",
    "\n",
    "\n",
    "# Unless you want to change to location, make sure the chromedriver program is located within the same file folder that you run this application in.\n",
    "# You must have chrome (or download another browser driver and change the path). Download the chrome software here: https://chromedriver.chromium.org/downloads\n",
    "path = os.getcwd() + \"/chromedriver\"\n",
    "driver = webdriver.Chrome(executable_path = path, options=options)\n",
    "# driver.maximize_window()\n",
    "\n",
    "# Demo if there is no URL entered\n",
    "if URL == '':\n",
    "    URL = r'https://ehrafworldcultures.yale.edu/search?q=text%3AApple&fq=culture_level_samples%7CPSF'\n",
    "\n",
    "homeURL = \"https://ehrafworldcultures.yale.edu/\"\n",
    "searchTokens = URL.split('/')[-1]\n",
    "\n",
    "# Load the HTML page (note that this should be updated to allow for modular input)\n",
    "driver.get(homeURL + searchTokens)\n",
    "\n",
    "# Find then click on each tab to reveal content for scraping\n",
    "# Elements must be individually clicked backwards. I do not know why this is a thing but my guess is each \n",
    "# clicked tab adds HTML pushing future tabs to a new location thereby making some indexing no longer point to a retrieved tab. \n",
    "# Loading backwards avoids this.\n",
    "country_tab = driver.find_elements_by_class_name('trad-overview__result')\n",
    "for i in range(len(country_tab)-1,-1,-1):\n",
    "    try:\n",
    "        #Note: this clicking should work for each of the Regions. However, technically, trad-overview__result is not the actual \n",
    "        # element that should be clicked on. It is just good enough for simplicity sake. If this give you trouble, consider putting in\n",
    "        # driver.execute_script(\"arguments[0].click();\", country_tab[i]) and changing the above drive.find to a button element or whatever is the true clickable drop down, \n",
    "        # although this will take a bit more indexing so beware.\n",
    "        country_tab[i].click()\n",
    "        \n",
    "    except:\n",
    "        print(f\"WARNING tab {i} failed to be clicked\")\n",
    "\n",
    "# Parse processed webpage with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "# extract the number of documents intended to be found\n",
    "document_count = soup.find_all(\"span\", {'class':'found__results'})\n",
    "document_count = document_count[0].small.em.next_element\n",
    "document_count = int(document_count.split()[1])\n",
    "# estimate the time this will take\n",
    "import math\n",
    "time_sec = document_count/4.33\n",
    "time_min = \"\"\n",
    "time_hour = \"\"\n",
    "if time_sec > 3600:\n",
    "    time_hour = math.floor(time_sec/3600)\n",
    "    time_sec -= time_hour*3600\n",
    "    time_hour = f\"{time_hour} hour(s), \"\n",
    "if time_sec > 60:\n",
    "    time_min = math.floor(time_sec/60)\n",
    "    time_sec -= time_min*60\n",
    "    time_min = f\"{time_min} minute(s), and \"\n",
    "\n",
    "time_sec = f\"{math.floor(time_sec)} second(s)\"\n",
    "\n",
    "\n",
    "print(f\"This will scrape up to {document_count} documents and take roughly \\n{time_hour}{time_min}{time_sec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cultures extracted 2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create a dictionary to store all cultures and their links for later use\n",
    "culture_dict = {}\n",
    "\n",
    "# find the tables containing the cultures then loop through them to extract their subregion, region, name, and the link to the passages\n",
    "# Note that if the ehraf website changes, this loop might need fixing by changing where the information is retrieved.\n",
    "# Also note that if the dynamic page is not loaded correctly, (a warning is given above), this may also fail.\n",
    "table_culture_links = soup.find_all('tr', {'class':'mdc-data-table__row'})\n",
    "\n",
    "# repeat in case the website took to long to load.\n",
    "loop_protect = 0\n",
    "while len(table_culture_links) == 0:\n",
    "    time.sleep(.1)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    table_culture_links = soup.find_all('tr', {'class':'mdc-data-table__row'})\n",
    "    loop_protect += 1\n",
    "    if loop_protect > 5:\n",
    "        raise Exception(f\"Repeated loading {loop_protect-1} times but did not find links\")\n",
    "for culture_i in table_culture_links:\n",
    "    culture_list = list(culture_i.children)\n",
    "\n",
    "    subRegion = culture_list[0].text\n",
    "    cultureName = culture_list[1].a.text\n",
    "    link = culture_list[1].a.attrs['href']\n",
    "    region = culture_i.findParent('table', {'role':'region'}).attrs['id']\n",
    "    source_count = int(culture_list[-2].text)\n",
    "    \n",
    "    culture_dict[cultureName] = {\"Region\":region, \"SubRegion\":subRegion, \"link\":link, \"Source_count\":source_count, \"Reloads\":{\"Source_reload\":0, \"Doc_reload\":0}}\n",
    "print(f\"Number of cultures extracted {len(culture_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382 documents out of a possible 382 loaded (also check dataframe)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_count_total = 0\n",
    "\n",
    "# create dataframe to hold all the data\n",
    "df_eHRAF = pd.DataFrame({\"Region\":[], \"SubRegion\":[], \"Culture\":[], 'DocTitle':[], 'Year':[], \"OCM\":[], \"OWC\":[], \"Passage\":[]})\n",
    "\n",
    "\n",
    "\n",
    "# For each Culture, go to their webpage link then scrape the document data\n",
    "for key in culture_dict.keys():\n",
    "    driver.get(homeURL + culture_dict[key]['link'])\n",
    "    # driver.get(homeURL + culture_dict['Azande']['link'])\n",
    "    # driver.get(homeURL + culture_dict[key]['link'])\n",
    "    doc_count = 0\n",
    "    \n",
    "    # dataframe for each culture\n",
    "    df_eHRAFCulture = pd.DataFrame({\"Region\":[], \"SubRegion\":[], \"Culture\":[], 'DocTitle':[], 'Year':[], \"OCM\":[], \"OWC\":[], \"Passage\":[]})\n",
    "\n",
    "    # loop until every page containing a source tab is clicked\n",
    "    source_total = culture_dict[key]['Source_count']\n",
    "    while source_total > 0:\n",
    "        # Try to make the program wait until the wepage is loaded\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"mdc-data-table__row\")))\n",
    "        #Click every source tab\n",
    "        sourceTabs = driver.find_elements_by_class_name('mdc-data-table__row')\n",
    "        for source_i in sourceTabs:\n",
    "            driver.execute_script(\"arguments[0].click();\", source_i)\n",
    "\n",
    "        #Log the source table's results number in order to know where to start and stop clicking.\n",
    "        # Skip every 2 logs as they do not contain the information desired\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        sourceCount = soup.find_all('td',{'class':'mdc-data-table__cell mdc-data-table__cell--numeric'})\n",
    "        sourceCount_list = list(map(lambda x: int(x.text), sourceCount[0::3]))\n",
    "\n",
    "\n",
    "        \n",
    "        # WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"trad-data__results\")))\n",
    "\n",
    "        # wait to make sure the page is loaded. CHANGE to a higher time if it runs indefinately\n",
    "        time.sleep(.1)\n",
    "\n",
    "        #get the results tab(which is basically the source tab but contained within a different HTML element) for sub indexing sources\n",
    "        resultsTabs = driver.find_elements_by_class_name('trad-data__results')\n",
    "        # if the resultsTabs did not all load, reload as necessary\n",
    "        reload_protect = 0\n",
    "        while len(sourceCount_list) != len(resultsTabs) and reload_protect<=10:\n",
    "            time.sleep(.1)\n",
    "            resultsTabs = driver.find_elements_by_class_name('trad-data__results')\n",
    "            reload_protect += 1\n",
    "        if reload_protect != 0:\n",
    "            culture_dict[key][\"Reloads\"][\"Source_reload\"] += reload_protect\n",
    "        \n",
    "\n",
    "        resultsTabs_count = len(resultsTabs) #For later reload checking\n",
    "\n",
    "        # click and extract information from each document within the result/source tabs\n",
    "        for i in range(len(resultsTabs)):\n",
    "            total = sourceCount_list[i]\n",
    "            \n",
    "            # loop until the program can click and find every piece of information for each document (this is probably where things will break if times are off)\n",
    "            while True:\n",
    "                docTabs = resultsTabs[i].find_elements_by_class_name('sre-result__title')\n",
    "                #Click all the tabs within a source\n",
    "                for doc in docTabs:\n",
    "                    driver.execute_script(\"arguments[0].click();\", doc)\n",
    "                    doc_count +=1\n",
    "\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source)\n",
    "                #Extract the document INFO here\n",
    "                soupDocs = soup.find_all('section',{'class':'sre-result__sre-result'}, limit=total)\n",
    "                for soupDoc in soupDocs:\n",
    "                    docPassage = soupDoc.find('div',{'class':'sre-result__sre-content'}).text\n",
    "                    \n",
    "                    soupOCM = soupDoc.find_all('div',{'class':'sre-result__ocms'})\n",
    "                    # OCMs\n",
    "                    # find all direct children a tags then extract the text\n",
    "                    ocmTags = soupOCM[0].find_all('a', recursive=False)\n",
    "                    OCM_list = []\n",
    "                    for ocmTag in ocmTags:\n",
    "                        OCM_list.append(int(ocmTag.span.text))\n",
    "                    # OWC\n",
    "                    OWC = soupOCM[1].a['name']\n",
    "\n",
    "                    DocTitle = soupDoc.find('div',{'class':'sre-result__sre-content-metadata'})\n",
    "                    DocTitle = DocTitle.div.text\n",
    "                    # Search for the document's year of creation \n",
    "                    Year = re.search('\\(([0-9]{0,4})\\)', DocTitle)\n",
    "                    if Year is not None:\n",
    "                        # remove the date then strip white space at the end and start to give the document's title\n",
    "                        DocTitle = re.sub(f'\\({Year.group()}\\)', '', DocTitle).strip()\n",
    "                        # get the year without the parenthesis\n",
    "                        Year = int(Year.group()[1:-1])\n",
    "                    \n",
    "                    # dataframe for each document\n",
    "                    df_Doc = pd.DataFrame({'OCM':[OCM_list], 'OWC':[OWC], 'DocTitle':[DocTitle], 'Year':[Year],  'Passage':[docPassage]})\n",
    "                    df_eHRAFCulture = pd.concat([df_eHRAFCulture, df_Doc], ignore_index=True)\n",
    "                # set remaining docs in a source tab (for clicking the \"next\" button if not all of them are shown)\n",
    "                total -= len(docTabs)\n",
    "\n",
    "                # If there are more tabs hidden away, find the button, click it, and then refresh the results\n",
    "                # otherwise, end the loop and close the source tab to make search for information easier\n",
    "                # NOTE that we have to search for the resultsTabs again because the page refreshed and the points \n",
    "                # originally found above no longer point to the same location and therefore will not work\n",
    "                if total >0:\n",
    "                    SourceTabFooter = resultsTabs[i].find_elements_by_class_name('trad-data__results--pagination')\n",
    "                    buttons = SourceTabFooter[0].find_elements_by_class_name('rmwc-icon--ligature')\n",
    "                    driver.execute_script(\"arguments[0].click();\", buttons[-1])\n",
    "                    time.sleep(.1)\n",
    "                    resultsTabs = driver.find_elements_by_class_name('trad-data__results')\n",
    "                    # in case .1 was not enough time, redo until the entire page is loaded again.\n",
    "                    reload_protect = 0\n",
    "                    while len(resultsTabs) < resultsTabs_count and reload_protect<=10:\n",
    "                        time.sleep(.1)\n",
    "                        resultsTabs = driver.find_elements_by_class_name('trad-data__results')\n",
    "                        reload_protect += 1\n",
    "                    # else:\n",
    "                    #     raise Exception(\"failed to load all results tabs, please contact ericchantland@gmail.com for info on fixing the time waits\")\n",
    "                    if reload_protect != 0:\n",
    "                        if reload_protect > 10:\n",
    "                            raise Exception(\"failed to load all results tabs, please contact ericchantland@gmail.com for info on fixing the time waits\")\n",
    "                        else:\n",
    "                            culture_dict[key][\"Reloads\"][\"Doc_reload\"] += reload_protect\n",
    "                            \n",
    "                else:\n",
    "                    ## close sourcetab(this might save time in the long run) \n",
    "                    ## NOTE: commented out because it will not work anymore with multi sources (sources with more than 10 passages). \n",
    "                    ## If you want it to close the tabs, you could copy the above resultsTabs reload and put it right after this line of code then chnage docTabs = resultsTabs[i] above to docTabs = resultsTabs[0]\n",
    "                    # driver.execute_script(\"arguments[0].click();\", sourceTabs[i])\n",
    "                    break\n",
    "        # Run to the next page if necessary. Check to see if there are more source tabs left, if so, click the next page and continue scraping the page\n",
    "        source_total -= len(resultsTabs)\n",
    "        if source_total >0:\n",
    "            next_page = driver.find_element_by_xpath(\"//button[@title='Next Page']\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "\n",
    "\n",
    "\n",
    "    df_eHRAFCulture[['Region','SubRegion',\"Culture\"]] = [culture_dict[key]['Region'], culture_dict[key]['SubRegion'], key ]    \n",
    "    df_eHRAF = pd.concat([df_eHRAF, df_eHRAFCulture], ignore_index=True)\n",
    "    doc_count_total += doc_count\n",
    "    if doc_count < sum(sourceCount_list):\n",
    "        print(f\"WARNING {doc_count} out of {sum(sourceCount_list)} documents loaded for {key}\")\n",
    "\n",
    "print(f'{doc_count_total} documents out of a possible {document_count} loaded (also check dataframe)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kuna  Source reloads:  0  Document reloads:  1\n",
      "Kapauku  Source reloads:  0  Document reloads:  1\n",
      "Lau Fijians  Source reloads:  0  Document reloads:  1\n",
      "Saramaka  Source reloads:  0  Document reloads:  1\n"
     ]
    }
   ],
   "source": [
    "# print reload count\n",
    "for key, val in culture_dict.items():\n",
    "    if culture_dict[key][\"Reloads\"][\"Source_reload\"] >0 or culture_dict[key][\"Reloads\"][\"Doc_reload\"] >0:\n",
    "        print(key,\" Source reloads: \", culture_dict[key][\"Reloads\"][\"Source_reload\"],\" Document reloads: \", culture_dict[key][\"Reloads\"][\"Doc_reload\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericchantland/Documents/GitHub/Boyer_eHRAF_Misery/eHRAF_Scraper.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericchantland/Documents/GitHub/Boyer_eHRAF_Misery/eHRAF_Scraper.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# close the webpage\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericchantland/Documents/GitHub/Boyer_eHRAF_Misery/eHRAF_Scraper.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m driver\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:688\u001b[0m, in \u001b[0;36mWebDriver.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclose\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    682\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[39m    Closes the current window.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[39m    :Usage:\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[39m        driver.close()\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mCLOSE)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:319\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    316\u001b[0m         params[\u001b[39m'\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[1;32m    318\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_value(params)\n\u001b[0;32m--> 319\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:374\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    372\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[1;32m    373\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url, path)\n\u001b[0;32m--> 374\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:397\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    394\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 397\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    399\u001b[0m     statuscode \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    400\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/request.py:74\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m urlopen_kw[\u001b[39m\"\u001b[39m\u001b[39mrequest_url\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m url\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_url_methods:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/request.py:96\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m fields:\n\u001b[1;32m     94\u001b[0m     url \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m urlencode(fields)\n\u001b[0;32m---> 96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# close the webpage\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no null values found\n"
     ]
    }
   ],
   "source": [
    "# Any null values?\n",
    "if df_eHRAF.isnull().values.any():\n",
    "    print('Some null values found:')\n",
    "    for col in df_eHRAF.columns:\n",
    "        print(f\"{col}: {df_eHRAF[col].isnull().sum()}\")\n",
    "else: \n",
    "    print(\"no null values found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get time and date that this program was run\n",
    "from datetime import date, datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%m/%d/%y\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text:(Apple+OR+pear+OR+banana)|PSF'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean and strip the URL to be put into the excel document\n",
    "\n",
    "replace_dict = {'%28':'(', '%29':')', '%3A':'~', '%7C':'|', '%3B':';'}\n",
    "remove_list = [homeURL, 'search', '\\?q=', 'fq=', '&', 'culture_level_samples']\n",
    "\n",
    "URL_name = URL\n",
    "\n",
    "for i in remove_list:\n",
    "    URL_name = re.sub(i, '', URL_name)\n",
    "# print(URL_name)\n",
    "for key, val in replace_dict.items():\n",
    "    URL_name = re.sub(key, val, URL_name)\n",
    "# print(URL_name)\n",
    "\n",
    "\n",
    "URL_name_nonPlussed = re.sub('\\+', ' ', URL_name)\n",
    "URL_name_nonPlussed = re.sub('\\~', ':', URL_name)\n",
    "URL_name_nonPlussed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>SubRegion</th>\n",
       "      <th>Culture</th>\n",
       "      <th>DocTitle</th>\n",
       "      <th>Year</th>\n",
       "      <th>OCM</th>\n",
       "      <th>OWC</th>\n",
       "      <th>Passage</th>\n",
       "      <th>run_Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Central Africa</td>\n",
       "      <td>Azande</td>\n",
       "      <td>Shifting cultivation in Africa: the Zande syst...</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>[245]</td>\n",
       "      <td>fo07</td>\n",
       "      <td>The most important natural enemy of bananas is...</td>\n",
       "      <td>User: Eric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Central Africa</td>\n",
       "      <td>Azande</td>\n",
       "      <td>Shifting cultivation in Africa: the Zande syst...</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>[241, 311]</td>\n",
       "      <td>fo07</td>\n",
       "      <td>PLATE 49. An old refuse heap which has been pl...</td>\n",
       "      <td>Run Time: 14:10:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Central Africa</td>\n",
       "      <td>Azande</td>\n",
       "      <td>Shifting cultivation in Africa: the Zande syst...</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>[245]</td>\n",
       "      <td>fo07</td>\n",
       "      <td>The Azande know several varieties of bananas a...</td>\n",
       "      <td>Run Date: 11/09/22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Central Africa</td>\n",
       "      <td>Azande</td>\n",
       "      <td>Shifting cultivation in Africa: the Zande syst...</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>[241]</td>\n",
       "      <td>fo07</td>\n",
       "      <td>(3) The old refuse heaps or guguda with rice, ...</td>\n",
       "      <td>Run Input: text:(Apple+OR+pear+OR+banana)|PSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Africa</td>\n",
       "      <td>Central Africa</td>\n",
       "      <td>Azande</td>\n",
       "      <td>Shifting cultivation in Africa: the Zande syst...</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>[241, 311]</td>\n",
       "      <td>fo07</td>\n",
       "      <td>There is much less cultivation in the courtyar...</td>\n",
       "      <td>Run URL: https://ehrafworldcultures.yale.edu/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>South-America</td>\n",
       "      <td>Southern South America</td>\n",
       "      <td>Mataco</td>\n",
       "      <td>The Mataco Indians and their language</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>[192, 196]</td>\n",
       "      <td>si07</td>\n",
       "      <td>One notices here the constant change of u to a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>South-America</td>\n",
       "      <td>Southern South America</td>\n",
       "      <td>Mataco</td>\n",
       "      <td>The Mataco Indians and their language</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>[192, 196]</td>\n",
       "      <td>si07</td>\n",
       "      <td>One notices here the constant change of u to a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>South-America</td>\n",
       "      <td>Southern South America</td>\n",
       "      <td>Ona</td>\n",
       "      <td>Analytical and critical bibliography of the tr...</td>\n",
       "      <td>1917.0</td>\n",
       "      <td>[104, 192, 197]</td>\n",
       "      <td>sh04</td>\n",
       "      <td>4. Throat. Sk, j[unknown] e[unknown] ka[unknow...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>South-America</td>\n",
       "      <td>Southern South America</td>\n",
       "      <td>Ona</td>\n",
       "      <td>Analytical and critical bibliography of the tr...</td>\n",
       "      <td>1917.0</td>\n",
       "      <td>[104, 192, 197]</td>\n",
       "      <td>sh04</td>\n",
       "      <td>4. Throat. Sk, j[unknown] e[unknown] ka[unknow...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>South-America</td>\n",
       "      <td>Southern South America</td>\n",
       "      <td>Ona</td>\n",
       "      <td>The Fireland Indians: Vol. 1. The Selk'nam, on...</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>[137, 222, 251, 252, 262, 462, 857]</td>\n",
       "      <td>sh04</td>\n",
       "      <td>During the warm third of the year, the Indians...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3731 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Region               SubRegion Culture  \\\n",
       "0            Africa          Central Africa  Azande   \n",
       "1            Africa          Central Africa  Azande   \n",
       "2            Africa          Central Africa  Azande   \n",
       "3            Africa          Central Africa  Azande   \n",
       "4            Africa          Central Africa  Azande   \n",
       "...             ...                     ...     ...   \n",
       "3726  South-America  Southern South America  Mataco   \n",
       "3727  South-America  Southern South America  Mataco   \n",
       "3728  South-America  Southern South America     Ona   \n",
       "3729  South-America  Southern South America     Ona   \n",
       "3730  South-America  Southern South America     Ona   \n",
       "\n",
       "                                               DocTitle    Year  \\\n",
       "0     Shifting cultivation in Africa: the Zande syst...  1956.0   \n",
       "1     Shifting cultivation in Africa: the Zande syst...  1956.0   \n",
       "2     Shifting cultivation in Africa: the Zande syst...  1956.0   \n",
       "3     Shifting cultivation in Africa: the Zande syst...  1956.0   \n",
       "4     Shifting cultivation in Africa: the Zande syst...  1956.0   \n",
       "...                                                 ...     ...   \n",
       "3726              The Mataco Indians and their language  1897.0   \n",
       "3727              The Mataco Indians and their language  1897.0   \n",
       "3728  Analytical and critical bibliography of the tr...  1917.0   \n",
       "3729  Analytical and critical bibliography of the tr...  1917.0   \n",
       "3730  The Fireland Indians: Vol. 1. The Selk'nam, on...  1931.0   \n",
       "\n",
       "                                      OCM   OWC  \\\n",
       "0                                   [245]  fo07   \n",
       "1                              [241, 311]  fo07   \n",
       "2                                   [245]  fo07   \n",
       "3                                   [241]  fo07   \n",
       "4                              [241, 311]  fo07   \n",
       "...                                   ...   ...   \n",
       "3726                           [192, 196]  si07   \n",
       "3727                           [192, 196]  si07   \n",
       "3728                      [104, 192, 197]  sh04   \n",
       "3729                      [104, 192, 197]  sh04   \n",
       "3730  [137, 222, 251, 252, 262, 462, 857]  sh04   \n",
       "\n",
       "                                                Passage  \\\n",
       "0     The most important natural enemy of bananas is...   \n",
       "1     PLATE 49. An old refuse heap which has been pl...   \n",
       "2     The Azande know several varieties of bananas a...   \n",
       "3     (3) The old refuse heaps or guguda with rice, ...   \n",
       "4     There is much less cultivation in the courtyar...   \n",
       "...                                                 ...   \n",
       "3726  One notices here the constant change of u to a...   \n",
       "3727  One notices here the constant change of u to a...   \n",
       "3728  4. Throat. Sk, j[unknown] e[unknown] ka[unknow...   \n",
       "3729  4. Throat. Sk, j[unknown] e[unknown] ka[unknow...   \n",
       "3730  During the warm third of the year, the Indians...   \n",
       "\n",
       "                                               run_Info  \n",
       "0                                            User: Eric  \n",
       "1                                    Run Time: 14:10:57  \n",
       "2                                    Run Date: 11/09/22  \n",
       "3         Run Input: text:(Apple+OR+pear+OR+banana)|PSF  \n",
       "4     Run URL: https://ehrafworldcultures.yale.edu/s...  \n",
       "...                                                 ...  \n",
       "3726                                               None  \n",
       "3727                                               None  \n",
       "3728                                               None  \n",
       "3729                                               None  \n",
       "3730                                               None  \n",
       "\n",
       "[3731 rows x 9 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# place run information within the \"run_info\" column\n",
    "df_eHRAF['run_Info'] = None\n",
    "df_eHRAF.loc[0, 'run_Info'] = \"User: \" + user\n",
    "df_eHRAF.loc[1, 'run_Info'] = \"Run Time: \" + str(current_time)\n",
    "df_eHRAF.loc[2, 'run_Info'] = \"Run Date: \" + str(current_date)\n",
    "df_eHRAF.loc[3, 'run_Info'] = \"Run Input: \" + URL_name_nonPlussed\n",
    "df_eHRAF.loc[4, 'run_Info'] = \"Run URL: \" + URL\n",
    "df_eHRAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_eHRAF.to_excel('Data/' + URL_name + '_web_data.xlsx', index=False)\n",
    "except:\n",
    "    print(\"Unable to save the title of the document, please rename it or risk overwriting\")\n",
    "    df_eHRAF.to_excel('Data/' + user + str(now.strftime(\"%m_%d_%y\")) + '_web_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = r'<button role=\"button\" tabindex=\"0\" class=\"rmwc-icon rmwc-icon--ligature material-icons mdc-ripple-upgraded--unbounded mdc-ripple-upgraded mdc-icon-button\" title=\"Next Page\" data-mdc-ripple-is-unbounded=\"true\" disabled=\"\" style=\"--mdc-ripple-fg-size:24px; --mdc-ripple-fg-scale:1.66667; --mdc-ripple-left:8px; --mdc-ripple-top:8px;\">keyboard_arrow_right</button>'\n",
    "y = r'<button role=\"button\" tabindex=\"0\" class=\"rmwc-icon rmwc-icon--ligature material-icons mdc-ripple-upgraded--unbounded mdc-ripple-upgraded mdc-icon-button\" title=\"Next Page\" data-mdc-ripple-is-unbounded=\"true\" style=\"--mdc-ripple-fg-size:24px; --mdc-ripple-fg-scale:1.66667; --mdc-ripple-left:8px; --mdc-ripple-top:8px;\">keyboard_arrow_right</button>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<button role=\"button\" tabindex=\"0\" class=\"rmwc-icon rmwc-icon--ligature material-icons mdc-ripple-upgraded--unbounded mdc-ripple-upgraded mdc-icon-button\" title=\"Next Page\" data-mdc-ripple-is-unbounded=\"true\" style=\"--mdc-ripple-fg-size:24px; --mdc-ripple-fg-scale:1.66667; --mdc-ripple-left:8px; --mdc-ripple-top:8px;\">keyboard_arrow_right</button>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3394e5ff5e37174aec0305e0ed7ec30b336f01a56a90646f493ecbcd8deec75b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
