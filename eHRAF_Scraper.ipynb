{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 # dataframe storing\n",
    "from bs4 import BeautifulSoup       # parsing web content in a nice way\n",
    "import ssl                          # MAY BE UNNECESSARY: provides access to the security socket layer (ssl) https://docs.python.org/3/library/ssl.html\n",
    "import urllib                       # open and navigate URL's\n",
    "import os\n",
    "\n",
    "from selenium import webdriver      # load and run the webpage dynamically.\n",
    "#from selenium.webdriver.chrome.options import Options\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a autonomous Chrome page to dynamically load the page for scraping. \n",
    "# Requires webdriver to be downloaded and then its path directed to.\n",
    "# Unless you want to change to location, make sure the chromedriver program is located within the same file folder that you run this application in.\n",
    "# You must have chrome (or a chromium base like internet explorer). Download software here: https://chromedriver.chromium.org/downloads\n",
    "path = os.getcwd() + \"/chromedriver\"\n",
    "driver = webdriver.Chrome(executable_path = path)\n",
    "\n",
    "homeURL = \"https://ehrafworldcultures.yale.edu/\"\n",
    "searchTokens = \"/search?q=text%3AApple&fq=culture_level_samples%7CPSF\"\n",
    "\n",
    "# Load the HTML page (note that this should be updated to allow for modular input)\n",
    "driver.get(homeURL + searchTokens)\n",
    "\n",
    "# Find then click on each tab to reveal content for scraping\n",
    "# Elements must be individually clicked backwards. I do not know why this is a thing but my guess is each \n",
    "# clicked tab adds HTML pushing future tabs to a new location thereby making some indexing no longer point to a retrieved tab. \n",
    "# Loading backwards avoids this.\n",
    "country_tab = driver.find_elements_by_class_name('trad-overview__result')\n",
    "for i in range(len(country_tab)-1,-1,-1):\n",
    "    try:\n",
    "        country_tab[i].click()\n",
    "    except:\n",
    "        print(f\"WARNING tab {i} failed to be clicked\")\n",
    "\n",
    "# Parse processed webpage with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Africa\n",
      "Asia\n",
      "Europe\n",
      "Middle America and the Caribbean\n",
      "Middle East\n",
      "North America\n",
      "Oceania\n",
      "South America\n"
     ]
    }
   ],
   "source": [
    "#Example of finding then printing out the dynamic webpage (before clicking). Shown here are the continents\n",
    "continent_dir = soup.find_all('div',\n",
    "{'class':'trad-overview__result'})\n",
    "\n",
    "for x in continent_dir:\n",
    "    print(x.h4.button.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cultures extracted 48\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store all cultures and their links for later use\n",
    "culture_dict = {}\n",
    "\n",
    "# find the tables containing the cultures then loop through them to extract their subregion, continent, name, and the link to the passages\n",
    "# Note that if the ehraf website changes, this loop might need fixing by changing where the information is retrieved.\n",
    "# Also note that if the dynamic page is not loaded correctly, (a warning is given above), this may also fail.\n",
    "table_culture_links = soup.find_all('tr', {'class':'mdc-data-table__row'})\n",
    "for culture_i in table_culture_links:\n",
    "    culture_list = list(culture_i.children)\n",
    "\n",
    "    subRegion = culture_list[0].text\n",
    "    cultureName = culture_list[1].a.text\n",
    "    link = culture_list[1].a.attrs['href']\n",
    "    continent = culture_i.findParent('table', {'role':'region'}).attrs['id']\n",
    "    # \n",
    "    culture_dict[cultureName] = {\"Continent\":continent, \"SubRegion\":subRegion, \"link\":link}\n",
    "print(f\"Number of cultures extracted {len(culture_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING sourceTab 0 failed to be clicked\n"
     ]
    },
    {
     "ename": "MaxRetryError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=53588): Max retries exceeded with url: /session/9559a0ea5136f8c1d137f5f2842295b2/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fed5995fa90>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[39mraise\u001b[39;00m socket\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mgetaddrinfo returns an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    400\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/http/client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/http/client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fed5995fa90>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericchantland/Documents/GitHub/Boyer_eHRAF_Misery/eHRAF_Scraper.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericchantland/Documents/GitHub/Boyer_eHRAF_Misery/eHRAF_Scraper.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m culture_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericchantland/Documents/GitHub/Boyer_eHRAF_Misery/eHRAF_Scraper.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m#navigate to a culture's page via the link\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericchantland/Documents/GitHub/Boyer_eHRAF_Misery/eHRAF_Scraper.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     driver\u001b[39m.\u001b[39;49mget(homeURL \u001b[39m+\u001b[39;49m culture_dict[key][\u001b[39m'\u001b[39;49m\u001b[39mlink\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericchantland/Documents/GitHub/Boyer_eHRAF_Misery/eHRAF_Scraper.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m#click through the source tabs\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericchantland/Documents/GitHub/Boyer_eHRAF_Misery/eHRAF_Scraper.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     sourceTab \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mfind_elements_by_class_name(\u001b[39m'\u001b[39m\u001b[39mmdc-data-table__row\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:333\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url):\n\u001b[1;32m    330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m    Loads a web page in the current browser session.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m'\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m'\u001b[39;49m: url})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:319\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    316\u001b[0m         params[\u001b[39m'\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[1;32m    318\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_value(params)\n\u001b[0;32m--> 319\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:374\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    372\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[1;32m    373\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url, path)\n\u001b[0;32m--> 374\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:397\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    394\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 397\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    399\u001b[0m     statuscode \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    400\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn:\n\u001b[1;32m    811\u001b[0m     \u001b[39m# Try again\u001b[39;00m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    832\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn:\n\u001b[1;32m    811\u001b[0m     \u001b[39m# Try again\u001b[39;00m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    832\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn:\n\u001b[1;32m    811\u001b[0m     \u001b[39m# Try again\u001b[39;00m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    832\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, (SocketError, HTTPException)):\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n\u001b[1;32m    792\u001b[0m \u001b[39m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    581\u001b[0m new_retry \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew(\n\u001b[1;32m    582\u001b[0m     total\u001b[39m=\u001b[39mtotal,\n\u001b[1;32m    583\u001b[0m     connect\u001b[39m=\u001b[39mconnect,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     history\u001b[39m=\u001b[39mhistory,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n\u001b[1;32m    596\u001b[0m \u001b[39mreturn\u001b[39;00m new_retry\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=53588): Max retries exceeded with url: /session/9559a0ea5136f8c1d137f5f2842295b2/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fed5995fa90>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "\n",
    "for key, val in culture_dict.items():\n",
    "    #navigate to a culture's page via the link\n",
    "    driver.get(homeURL + culture_dict[key]['link'])\n",
    "\n",
    "    #click through the source tabs\n",
    "    sourceTab = driver.find_elements_by_class_name('mdc-data-table__row')\n",
    "    for i in range(len(sourceTab)-1,-1,-1):\n",
    "        try:\n",
    "            sourceTab[i].click()\n",
    "        except:\n",
    "            print(f\"WARNING sourceTab {i} failed to be clicked\")\n",
    "    \n",
    "    #Log the source table's results number in order to know where to start and stop clicking.\n",
    "    # Skip every 2 logs as they do not contain the information desired\n",
    "    sourceCount = soup.find_all('td',{'class':'mdc-data-table__cell mdc-data-table__cell--numeric'})\n",
    "    sourceCount_list = list(map(lambda x: int(x.text), sourceCount[0::3]))\n",
    "\n",
    "    #go through each source\n",
    "    # pastTabs is to help with indexing so the clicker does not click tabs already clicked\n",
    "    pastTabs = 0\n",
    "    for i in sourceCount_list:\n",
    "        total = i #superfluous\n",
    "        while total > 0:\n",
    "            #mark out the number of tabs that will be clicked\n",
    "            if total >= 10:\n",
    "                log = 10\n",
    "            else:\n",
    "                log = total\n",
    "            total = total - log\n",
    "\n",
    "            \n",
    "            docTab = driver.find_elements_by_class_name('trad-data__results--row')\n",
    "            for i in range(log+pastTabs-1,pastTabs-1,-1):\n",
    "                try:\n",
    "                    docTab[i].click()\n",
    "                except:\n",
    "                    print(f\"WARNING docTab {i} failed to be clicked\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# driver.get("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "log = 6\n",
    "pastTabs = 0\n",
    "x = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for i in range(log+pastTabs-1,pastTabs-1,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">1</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">1926</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">not specified</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">4</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">1937</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">1926-1929</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">1</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">1956</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">not specified</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">2</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">1971</td>,\n",
       " <td class=\"mdc-data-table__cell mdc-data-table__cell--numeric\">ca. 1863 - 1930</td>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_table = soup.find_all('td',{'class':'mdc-data-table__cell--numeric'})\n",
    "source_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "#A way to retrieve the results list\n",
    "source_table = soup.find_all('td',{'class':'mdc-data-table__cell mdc-data-table__cell--numeric'})\n",
    "source_table_list = list(map(lambda x: int(x.text), source_table[0::3]))\n",
    "print(source_table_list)\n",
    "# for i in source_table[0::3]:\n",
    "#     print(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy example for clicking links ion the next page.\n",
    "driver.get(homeURL + culture_dict['Azande']['link'])\n",
    "sourceTab = driver.find_elements_by_class_name('mdc-data-table__row')\n",
    "for i in range(len(sourceTab)-1,-1,-1):\n",
    "    try:\n",
    "        sourceTab[i].click()\n",
    "    except:\n",
    "        print(f\"WARNING tab {i} failed to be clicked\")\n",
    "        \n",
    "soup = BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(homeURL + culture_dict['Azande']['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"1eeb63906685dccc9e96c7be4121e53d\", element=\"e52c8c44-d05f-4048-b7bd-19e4645e4d80\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1eeb63906685dccc9e96c7be4121e53d\", element=\"3908a8fb-f2cb-4e38-ade6-637a072040a3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1eeb63906685dccc9e96c7be4121e53d\", element=\"6980db78-7e48-4e65-88cb-926f9ca0cace\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1eeb63906685dccc9e96c7be4121e53d\", element=\"90b49251-6d02-4dbb-a4e9-ab625d722592\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1eeb63906685dccc9e96c7be4121e53d\", element=\"86ff4a41-da13-4ec5-acc4-b52b44d4a6a9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1eeb63906685dccc9e96c7be4121e53d\", element=\"13281286-645b-4ca5-9889-899d1ecf1f3f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1eeb63906685dccc9e96c7be4121e53d\", element=\"6051bcac-f673-4fa8-bd1f-f8362fe1598e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1eeb63906685dccc9e96c7be4121e53d\", element=\"167d566a-c290-4818-afe8-506a0e74179b\")>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsTab = driver.find_elements_by_class_name('trad-data__results--row')\n",
    "resultsTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"1eeb63906685dccc9e96c7be4121e53d\", element=\"e52c8c44-d05f-4048-b7bd-19e4645e4d80\")>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsTab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "x = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "print(x[1:11])\n",
    "print(len(x[1:11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_table = soup.find_all('tr',{'class':'mdc-data-table__row'})\n",
    "len(document_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bs4.element as element\n",
    "# body_child_tags = list(filter( lambda x: type(x) == element.Tag, country_links)) #go through, if there are tags, create a list then print\n",
    "# body_child_tags "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3394e5ff5e37174aec0305e0ed7ec30b336f01a56a90646f493ecbcd8deec75b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
