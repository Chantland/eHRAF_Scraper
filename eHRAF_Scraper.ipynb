{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraper\n",
    "\n",
    "#### **Author** -- Eric Chantland (ericchantland@gmail.com)\n",
    "#### **Created** -- October 2022\n",
    "\n",
    "\n",
    "<p>The objective of this program is to webscrape the eHRAF database for various text passages in order to explore misery and how it relates with folk/wild traditions. To run this program, merely press play and enter in the URL and your name. To obtain the URL, progress through ehRAF website and enter in your search terms in the \"advanced search\" boxes and the filters. When you have successfully filtered the searches (so the region list) but BEFORE you actually look at any of the culture's links, copy the URL that it gives you into this program. This program seeks to scrape the webpage for texts and OCM's. It is therefore very susceptible to the eHRAF webpage changing, loading slow/fast, and/or not providing standardized information. If this program fails, it is likely due to one of these three.<p>\n",
    "\n",
    "<p> To run this program, you can go individually or just run all the cells at once. However, this autonomous webpage (chrome) must be running for the rest of the webscraper to work. Therefore, if you run into an issue, just rerurn the whole program. If that does not fix it, then there must be a different issue. <p>\n",
    "\n",
    "<p> If the program stalls for whatever reason, then webpage may have loaded too slow and the scraper was not able to catch that this happened resulting in an infinite loop. The program needs to be updated for its sleep timer. <b>Stop the program and contact me.<b> <p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages requirements\n",
    "\n",
    "If you are loading this for the first time, you will need the packages used in the file. The best way is to use a virtual environment or use anaconda (which might save on space but some package may not be available through there). For a virtual environment (venv) type into the terminal:\n",
    "\n",
    "        python - m venv venv\n",
    "        \n",
    "When venv is created, select it as your prefered kernal (should give you a prompt to do so, otherwise, select it in the to right corner). Then create a new terminal (should be able to do so at the top of the mac screen under \"terminal\"). Each line in your terminal should start with \"venv\" or whatever you named the environment if you called it something else. Now you can install the requirments in the terminal using:\n",
    "\n",
    "        pip install -r requirements.txt -v\n",
    "        \n",
    "NOTE: It is likely the requirements file is bloated with packages you do not need. I have not tried to slim it down so feel free to just install packages as you see fit with\n",
    "\n",
    "        pip install <your package name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 # dataframe storing\n",
    "from bs4 import BeautifulSoup       # parsing web content in a nice way\n",
    "import ssl                          # MAY BE UNNECESSARY: provides access to the security socket layer (ssl) https://docs.python.org/3/library/ssl.html\n",
    "import urllib                       # MAY BE UNNECESSARY: open and navigate URL's\n",
    "import os                           # Find where this file is located.\n",
    "import time                         # for pausing the program in order for it to load the webpage\n",
    "import re                           # regex for searching through strings\n",
    "\n",
    "\n",
    "from selenium import webdriver      # load and run the webpage dynamically.\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# for wait times\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL\n",
    "\n",
    "\n",
    "<b>You will need to use your browser for this.</b> On eHRAF homepage, you may put in various search terms and queries using their GUI (their menu) once you are happy with the search terms,\n",
    "To access this menu, go to https://ehrafworldcultures.yale.edu/search/advanced where you will see a section for \"culture\", \"subjects\" and \"keywords\". You may input words separated by a comma into any and all of these boxes to conduct your search. Addiitonally, you can click the boixes which say \"Any of these ___\" to choose if you want \"any\" \"none\" or \"all\" of the search terms you put in the specific box. When you think it looks good, press \"search\" and you will get a URL at the top of your browser. <b> However</b> you may also filter the document further in the main page (shown in the picture below) by clicking on the \"filter\" button. A common filter to use is checking the \"culture Level Samples\" tab and selecting \"PSF\". This will update the URL which you can paste\n",
    "\n",
    "<p> Try getting the same URL as shown below. We advanced searched for the keyword \"apple\" and chose the \"culture level sample\" of \"PSF\" in the filter </p>\n",
    "\n",
    " Copy and paste the URL into the parenthesis of the Variable below:\n",
    "For example, to get the search for all documents containing \"apple\" and through the PSF, I got a hyperlink like this:\n",
    "https://ehrafworldcultures.yale.edu/search?q=text%3AApple&fq=culture_level_samples%7CPSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the hyperlink goes within quotes!\n",
    "URL = input(\"Please enter in an eHRAF URL, otherwise enter nothing for a demo\\n\")\n",
    "user = input(\"What is your name?\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"Hyperlink-Example.png\"> -->\n",
    "![alt text](Hyperlink-Example.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SessionNotCreatedException",
     "evalue": "Message: session not created: This version of ChromeDriver only supports Chrome version 106\nCurrent browser version is 108.0.5359.71 with binary path /Applications/Google Chrome.app/Contents/MacOS/Google Chrome\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Unless you want to change to location, make sure the chromedriver program is located within the same file folder that you run this application in.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# You must have chrome (or download another browser driver and change the path). Download the chrome software here: https://chromedriver.chromium.org/downloads\u001b[39;00m\n\u001b[1;32m     12\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetcwd() \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/chromedriver\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mChrome(executable_path \u001b[39m=\u001b[39;49m path, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m     14\u001b[0m \u001b[39m# driver.maximize_window()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[39m# Demo if there is no URL entered\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m URL \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py:76\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     RemoteWebDriver\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     77\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m     78\u001b[0m         command_executor\u001b[39m=\u001b[39;49mChromeRemoteConnection(\n\u001b[1;32m     79\u001b[0m             remote_server_addr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice\u001b[39m.\u001b[39;49mservice_url,\n\u001b[1;32m     80\u001b[0m             keep_alive\u001b[39m=\u001b[39;49mkeep_alive),\n\u001b[1;32m     81\u001b[0m         desired_capabilities\u001b[39m=\u001b[39;49mdesired_capabilities)\n\u001b[1;32m     82\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquit()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:157\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, command_executor, desired_capabilities, browser_profile, proxy, keep_alive, file_detector, options)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m browser_profile \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mPlease use FirefoxOptions to set browser profile\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    156\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_session(capabilities, browser_profile)\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to \u001b[39m=\u001b[39m SwitchTo(\u001b[39mself\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mobile \u001b[39m=\u001b[39m Mobile(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:252\u001b[0m, in \u001b[0;36mWebDriver.start_session\u001b[0;34m(self, capabilities, browser_profile)\u001b[0m\n\u001b[1;32m    249\u001b[0m w3c_caps \u001b[39m=\u001b[39m _make_w3c_caps(capabilities)\n\u001b[1;32m    250\u001b[0m parameters \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcapabilities\u001b[39m\u001b[39m\"\u001b[39m: w3c_caps,\n\u001b[1;32m    251\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mdesiredCapabilities\u001b[39m\u001b[39m\"\u001b[39m: capabilities}\n\u001b[0;32m--> 252\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mNEW_SESSION, parameters)\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m response:\n\u001b[1;32m    254\u001b[0m     response \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:321\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> 321\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    322\u001b[0m     response[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(\n\u001b[1;32m    323\u001b[0m         response\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    324\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eHRAF_Scraper/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:242\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m'\u001b[39m\u001b[39malert\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)\n\u001b[0;32m--> 242\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mSessionNotCreatedException\u001b[0m: Message: session not created: This version of ChromeDriver only supports Chrome version 106\nCurrent browser version is 108.0.5359.71 with binary path /Applications/Google Chrome.app/Contents/MacOS/Google Chrome\n"
     ]
    }
   ],
   "source": [
    "# Use a autonomous Chrome page to dynamically load the page for scraping. \n",
    "# Requires webdriver to be downloaded and then its path directed to.\n",
    "\n",
    "# iniate \"headless\" which stops chrome from showing itself when this is run, \n",
    "# switch headless to False if you want to see the webpage or True if you want it to run in the background\n",
    "options = Options()\n",
    "options.headless = False\n",
    "\n",
    "\n",
    "# Unless you want to change to location, make sure the chromedriver program is located within the same file folder that you run this application in.\n",
    "# You must have chrome (or download another browser driver and change the path). Download the chrome software here: https://chromedriver.chromium.org/downloads\n",
    "path = os.getcwd() + \"/chromedriver\"\n",
    "driver = webdriver.Chrome(executable_path = path, options=options)\n",
    "# driver.maximize_window()\n",
    "\n",
    "# Demo if there is no URL entered\n",
    "if URL == '':\n",
    "    URL = r'https://ehrafworldcultures.yale.edu/search?q=text%3AApple&fq=culture_level_samples%7CPSF'\n",
    "\n",
    "homeURL = \"https://ehrafworldcultures.yale.edu/\"\n",
    "searchTokens = URL.split('/')[-1]\n",
    "\n",
    "# Load the HTML page (note that this should be updated to allow for modular input)\n",
    "driver.get(homeURL + searchTokens)\n",
    "\n",
    "# Find then click on each tab to reveal content for scraping\n",
    "# Elements must be individually clicked backwards. I do not know why this is a thing but my guess is each \n",
    "# clicked tab adds HTML pushing future tabs to a new location thereby making some indexing no longer point to a retrieved tab. \n",
    "# Loading backwards avoids this.\n",
    "country_tab = driver.find_elements_by_class_name('trad-overview__result')\n",
    "for i in range(len(country_tab)-1,-1,-1):\n",
    "    try:\n",
    "        #Note: this clicking should work for each of the Regions. However, technically, trad-overview__result is not the actual \n",
    "        # element that should be clicked on. It is just good enough for simplicity sake. If this give you trouble, consider putting in\n",
    "        # driver.execute_script(\"arguments[0].click();\", country_tab[i]) and changing the above drive.find to a button element or whatever is the true clickable drop down, \n",
    "        # although this will take a bit more indexing so beware.\n",
    "        country_tab[i].click()\n",
    "        \n",
    "    except:\n",
    "        print(f\"WARNING tab {i} failed to be clicked\")\n",
    "\n",
    "# Parse processed webpage with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "# extract the number of documents intended to be found\n",
    "document_count = soup.find_all(\"span\", {'class':'found__results'})\n",
    "document_count = document_count[0].small.em.next_element\n",
    "document_count = int(document_count.split()[1])\n",
    "# estimate the time this will take\n",
    "import math\n",
    "time_sec = document_count/4.33\n",
    "time_min = \"\"\n",
    "time_hour = \"\"\n",
    "if time_sec > 3600:\n",
    "    time_hour = math.floor(time_sec/3600)\n",
    "    time_sec -= time_hour*3600\n",
    "    time_hour = f\"{time_hour} hour(s), \"\n",
    "if time_sec > 60:\n",
    "    time_min = math.floor(time_sec/60)\n",
    "    time_sec -= time_min*60\n",
    "    time_min = f\"{time_min} minute(s), and \"\n",
    "\n",
    "time_sec = f\"{math.floor(time_sec)} second(s)\"\n",
    "\n",
    "\n",
    "print(f\"This will scrape up to {document_count} documents and take roughly \\n{time_hour}{time_min}{time_sec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cultures extracted 1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Create a dictionary to store all cultures and their links for later use\n",
    "culture_dict = {}\n",
    "\n",
    "# find the tables containing the cultures then loop through them to extract their subregion, region, name, and the link to the passages\n",
    "# Note that if the ehraf website changes, this loop might need fixing by changing where the information is retrieved.\n",
    "# Also note that if the dynamic page is not loaded correctly, (a warning is given above), this may also fail.\n",
    "table_culture_links = soup.find_all('tr', {'class':'mdc-data-table__row'})\n",
    "\n",
    "# repeat in case the website took to long to load.\n",
    "loop_protect = 0\n",
    "while len(table_culture_links) == 0:\n",
    "    time.sleep(.1)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    table_culture_links = soup.find_all('tr', {'class':'mdc-data-table__row'})\n",
    "    loop_protect += 1\n",
    "    if loop_protect > 5:\n",
    "        raise Exception(f\"Repeated loading {loop_protect-1} times but did not find links\")\n",
    "for culture_i in table_culture_links:\n",
    "    culture_list = list(culture_i.children)\n",
    "\n",
    "    subRegion = culture_list[0].text\n",
    "    cultureName = culture_list[1].a.text\n",
    "    link = culture_list[1].a.attrs['href']\n",
    "    region = culture_i.findParent('table', {'role':'region'}).attrs['id']\n",
    "    source_count = int(culture_list[-2].text)\n",
    "    \n",
    "    culture_dict[cultureName] = {\"Region\":region, \"SubRegion\":subRegion, \"link\":link, \"Source_count\":source_count, \"Reloads\":{\"Source_reload\":0, \"Doc_reload\":0}}\n",
    "print(f\"Number of cultures extracted {len(culture_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents out of a possible 4 loaded (also check dataframe)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_count_total = 0\n",
    "\n",
    "# create dataframe to hold all the data\n",
    "df_eHRAF = pd.DataFrame({\"Region\":[], \"SubRegion\":[], \"Culture\":[], 'DocTitle':[], 'Year':[], \"OCM\":[], \"OWC\":[], \"Passage\":[]})\n",
    "\n",
    "\n",
    "\n",
    "# For each Culture, go to their webpage link then scrape the document data\n",
    "for key in culture_dict.keys():\n",
    "    driver.get(homeURL + culture_dict[key]['link'])\n",
    "    # driver.get(homeURL + culture_dict['Azande']['link'])\n",
    "    # driver.get(homeURL + culture_dict[key]['link'])\n",
    "    doc_count = 0\n",
    "    \n",
    "    # dataframe for each culture\n",
    "    df_eHRAFCulture = pd.DataFrame({\"Region\":[], \"SubRegion\":[], \"Culture\":[], 'DocTitle':[], 'Year':[], \"OCM\":[], \"OWC\":[], \"Passage\":[]})\n",
    "\n",
    "    # loop until every page containing a source tab is clicked\n",
    "    source_total = culture_dict[key]['Source_count']\n",
    "    while source_total > 0:\n",
    "        # Try to make the program wait until the wepage is loaded\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"mdc-data-table__row\")))\n",
    "        #Click every source tab\n",
    "        sourceTabs = driver.find_elements_by_class_name('mdc-data-table__row')\n",
    "        for source_i in sourceTabs:\n",
    "            driver.execute_script(\"arguments[0].click();\", source_i)\n",
    "\n",
    "        #Log the source table's results number in order to know where to start and stop clicking.\n",
    "        # Skip every 2 logs as they do not contain the information desired\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        sourceCount = soup.find_all('td',{'class':'mdc-data-table__cell mdc-data-table__cell--numeric'})\n",
    "        sourceCount_list = list(map(lambda x: int(x.text), sourceCount[0::3]))\n",
    "\n",
    "\n",
    "        \n",
    "        # WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"trad-data__results\")))\n",
    "\n",
    "        # wait to make sure the page is loaded. CHANGE to a higher time if it runs indefinately\n",
    "        time.sleep(.1)\n",
    "\n",
    "        #get the results tab(which is basically the source tab but contained within a different HTML element) for sub indexing sources\n",
    "        resultsTabs = driver.find_elements_by_class_name('trad-data__results')\n",
    "        # if the resultsTabs did not all load, reload as necessary\n",
    "        reload_protect = 0\n",
    "        while len(sourceCount_list) != len(resultsTabs) and reload_protect<=10:\n",
    "            time.sleep(.1)\n",
    "            resultsTabs = driver.find_elements_by_class_name('trad-data__results')\n",
    "            reload_protect += 1\n",
    "        if reload_protect != 0:\n",
    "            culture_dict[key][\"Reloads\"][\"Source_reload\"] += reload_protect\n",
    "        \n",
    "\n",
    "        resultsTabs_count = len(resultsTabs) #For later reload checking\n",
    "\n",
    "        # click and extract information from each document within the result/source tabs\n",
    "        for i in range(len(resultsTabs)):\n",
    "            total = sourceCount_list[i]\n",
    "            \n",
    "            # loop until the program can click and find every piece of information for each document (this is probably where things will break if times are off)\n",
    "            while True:\n",
    "                docTabs = resultsTabs[i].find_elements_by_class_name('sre-result__title')\n",
    "                #Click all the tabs within a source\n",
    "                for doc in docTabs:\n",
    "                    driver.execute_script(\"arguments[0].click();\", doc)\n",
    "                    doc_count +=1\n",
    "\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source)\n",
    "                #Extract the document INFO here\n",
    "                soupDocs = soup.find_all('section',{'class':'sre-result__sre-result'}, limit=total)\n",
    "                for soupDoc in soupDocs:\n",
    "                    docPassage = soupDoc.find('div',{'class':'sre-result__sre-content'}).text\n",
    "                    \n",
    "                    soupOCM = soupDoc.find_all('div',{'class':'sre-result__ocms'})\n",
    "                    # OCMs\n",
    "                    # find all direct children a tags then extract the text\n",
    "                    ocmTags = soupOCM[0].find_all('a', recursive=False)\n",
    "                    OCM_list = []\n",
    "                    for ocmTag in ocmTags:\n",
    "                        OCM_list.append(int(ocmTag.span.text))\n",
    "                    # OWC\n",
    "                    OWC = soupOCM[1].a['name']\n",
    "\n",
    "                    DocTitle = soupDoc.find('div',{'class':'sre-result__sre-content-metadata'})\n",
    "                    DocTitle = DocTitle.div.text\n",
    "                    # Search for the document's year of creation \n",
    "                    Year = re.search('\\(([0-9]{0,4})\\)', DocTitle)\n",
    "                    if Year is not None:\n",
    "                        # remove the date then strip white space at the end and start to give the document's title\n",
    "                        DocTitle = re.sub(f'\\({Year.group()}\\)', '', DocTitle).strip()\n",
    "                        # get the year without the parenthesis\n",
    "                        Year = int(Year.group()[1:-1])\n",
    "                    \n",
    "                    # dataframe for each document\n",
    "                    df_Doc = pd.DataFrame({'OCM':[OCM_list], 'OWC':[OWC], 'DocTitle':[DocTitle], 'Year':[Year],  'Passage':[docPassage]})\n",
    "                    df_eHRAFCulture = pd.concat([df_eHRAFCulture, df_Doc], ignore_index=True)\n",
    "                # set remaining docs in a source tab (for clicking the \"next\" button if not all of them are shown)\n",
    "                total -= len(docTabs)\n",
    "\n",
    "                # If there are more tabs hidden away, find the button, click it, and then refresh the results\n",
    "                # otherwise, end the loop and close the source tab to make search for information easier\n",
    "                # NOTE that we have to search for the resultsTabs again because the page refreshed and the points \n",
    "                # originally found above no longer point to the same location and therefore will not work\n",
    "                if total >0:\n",
    "                    SourceTabFooter = resultsTabs[i].find_elements_by_class_name('trad-data__results--pagination')\n",
    "                    buttons = SourceTabFooter[0].find_elements_by_class_name('rmwc-icon--ligature')\n",
    "                    driver.execute_script(\"arguments[0].click();\", buttons[-1])\n",
    "                    time.sleep(.1)\n",
    "                    resultsTabs = driver.find_elements_by_class_name('trad-data__results')\n",
    "                    # in case .1 was not enough time, redo until the entire page is loaded again.\n",
    "                    reload_protect = 0\n",
    "                    while len(resultsTabs) < resultsTabs_count and reload_protect<=10:\n",
    "                        time.sleep(.1)\n",
    "                        resultsTabs = driver.find_elements_by_class_name('trad-data__results')\n",
    "                        reload_protect += 1\n",
    "                    # else:\n",
    "                    #     raise Exception(\"failed to load all results tabs, please contact ericchantland@gmail.com for info on fixing the time waits\")\n",
    "                    if reload_protect != 0:\n",
    "                        if reload_protect > 10:\n",
    "                            raise Exception(\"failed to load all results tabs, please contact ericchantland@gmail.com for info on fixing the time waits\")\n",
    "                        else:\n",
    "                            culture_dict[key][\"Reloads\"][\"Doc_reload\"] += reload_protect\n",
    "                            \n",
    "                else:\n",
    "                    ## close sourcetab(this might save time in the long run) \n",
    "                    ## NOTE: commented out because it will not work anymore with multi sources (sources with more than 10 passages). \n",
    "                    ## If you want it to close the tabs, you could copy the above resultsTabs reload and put it right after this line of code then chnage docTabs = resultsTabs[i] above to docTabs = resultsTabs[0]\n",
    "                    # driver.execute_script(\"arguments[0].click();\", sourceTabs[i])\n",
    "                    break\n",
    "        # Run to the next page if necessary. Check to see if there are more source tabs left, if so, click the next page and continue scraping the page\n",
    "        source_total -= len(resultsTabs)\n",
    "        if source_total >0:\n",
    "            next_page = driver.find_element_by_xpath(\"//button[@title='Next Page']\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_page)\n",
    "\n",
    "\n",
    "\n",
    "    df_eHRAFCulture[['Region','SubRegion',\"Culture\"]] = [culture_dict[key]['Region'], culture_dict[key]['SubRegion'], key ]    \n",
    "    df_eHRAF = pd.concat([df_eHRAF, df_eHRAFCulture], ignore_index=True)\n",
    "    doc_count_total += doc_count\n",
    "    if doc_count < sum(sourceCount_list):\n",
    "        print(f\"WARNING {doc_count} out of {sum(sourceCount_list)} documents loaded for {key}\")\n",
    "\n",
    "print(f'{doc_count_total} documents out of a possible {document_count} loaded (also check dataframe)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print reload count\n",
    "for key, val in culture_dict.items():\n",
    "    if culture_dict[key][\"Reloads\"][\"Source_reload\"] >0 or culture_dict[key][\"Reloads\"][\"Doc_reload\"] >0:\n",
    "        print(key,\" Source reloads: \", culture_dict[key][\"Reloads\"][\"Source_reload\"],\" Document reloads: \", culture_dict[key][\"Reloads\"][\"Doc_reload\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the webpage\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no null values found\n"
     ]
    }
   ],
   "source": [
    "# Any null values?\n",
    "if df_eHRAF.isnull().values.any():\n",
    "    print('Some null values found:')\n",
    "    for col in df_eHRAF.columns:\n",
    "        print(f\"{col}: {df_eHRAF[col].isnull().sum()}\")\n",
    "else: \n",
    "    print(\"no null values found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get time and date that this program was run\n",
    "from datetime import date, datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "current_date = now.strftime(\"%m/%d/%y\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cultures:%22Hawaiians%22+AND+(subjects:%22spirits+and+gods%22+AND+text:Apple)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean and strip the URL to be put into the excel document\n",
    "\n",
    "replace_dict = {'%28':'(', '%29':')', '%3A':'~', '%7C':'|', '%3B':';'}\n",
    "remove_list = [homeURL, 'search', '\\?q=', 'fq=', '&', 'culture_level_samples']\n",
    "\n",
    "URL_name = URL\n",
    "\n",
    "for i in remove_list:\n",
    "    URL_name = re.sub(i, '', URL_name)\n",
    "# print(URL_name)\n",
    "for key, val in replace_dict.items():\n",
    "    URL_name = re.sub(key, val, URL_name)\n",
    "# print(URL_name)\n",
    "\n",
    "\n",
    "URL_name_nonPlussed = re.sub('\\+', ' ', URL_name)\n",
    "URL_name_nonPlussed = re.sub('\\~', ':', URL_name)\n",
    "URL_name_nonPlussed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>SubRegion</th>\n",
       "      <th>Culture</th>\n",
       "      <th>DocTitle</th>\n",
       "      <th>Year</th>\n",
       "      <th>OCM</th>\n",
       "      <th>OWC</th>\n",
       "      <th>Passage</th>\n",
       "      <th>run_Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "      <td>Hawaiians</td>\n",
       "      <td>Arts and crafts of Hawaii</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>[322, 5311, 776, 778]</td>\n",
       "      <td>ov05</td>\n",
       "      <td>Further elaboration is shown in an image in th...</td>\n",
       "      <td>User: Eric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "      <td>Hawaiians</td>\n",
       "      <td>Arts and crafts of Hawaii</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>[322, 5311, 776, 778]</td>\n",
       "      <td>ov05</td>\n",
       "      <td>Further elaboration is shown in an image in th...</td>\n",
       "      <td>Run Time: 15:42:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "      <td>Hawaiians</td>\n",
       "      <td>Native planters in old Hawaii: their life, lor...</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>[533, 535, 776, 778, 824, 874]</td>\n",
       "      <td>ov05</td>\n",
       "      <td>Laka, the goddess of the wildwood who was patr...</td>\n",
       "      <td>Run Date: 11/29/22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "      <td>Hawaiians</td>\n",
       "      <td>Arts and crafts of Hawaii</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>[322, 5311, 776, 778]</td>\n",
       "      <td>ov05</td>\n",
       "      <td>Further elaboration is shown in an image in th...</td>\n",
       "      <td>Run Input: cultures:%22Hawaiians%22+AND+(subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Run URL: https://ehrafworldcultures.yale.edu/s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Region  SubRegion    Culture  \\\n",
       "0  Oceania  Polynesia  Hawaiians   \n",
       "1  Oceania  Polynesia  Hawaiians   \n",
       "2  Oceania  Polynesia  Hawaiians   \n",
       "3  Oceania  Polynesia  Hawaiians   \n",
       "4      NaN        NaN        NaN   \n",
       "\n",
       "                                            DocTitle    Year  \\\n",
       "0                          Arts and crafts of Hawaii  1957.0   \n",
       "1                          Arts and crafts of Hawaii  1957.0   \n",
       "2  Native planters in old Hawaii: their life, lor...  1972.0   \n",
       "3                          Arts and crafts of Hawaii  1957.0   \n",
       "4                                                NaN     NaN   \n",
       "\n",
       "                              OCM   OWC  \\\n",
       "0           [322, 5311, 776, 778]  ov05   \n",
       "1           [322, 5311, 776, 778]  ov05   \n",
       "2  [533, 535, 776, 778, 824, 874]  ov05   \n",
       "3           [322, 5311, 776, 778]  ov05   \n",
       "4                             NaN   NaN   \n",
       "\n",
       "                                             Passage  \\\n",
       "0  Further elaboration is shown in an image in th...   \n",
       "1  Further elaboration is shown in an image in th...   \n",
       "2  Laka, the goddess of the wildwood who was patr...   \n",
       "3  Further elaboration is shown in an image in th...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            run_Info  \n",
       "0                                         User: Eric  \n",
       "1                                 Run Time: 15:42:21  \n",
       "2                                 Run Date: 11/29/22  \n",
       "3  Run Input: cultures:%22Hawaiians%22+AND+(subje...  \n",
       "4  Run URL: https://ehrafworldcultures.yale.edu/s...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# place run information within the \"run_info\" column\n",
    "df_eHRAF['run_Info'] = None\n",
    "df_eHRAF.loc[0, 'run_Info'] = \"User: \" + user\n",
    "df_eHRAF.loc[1, 'run_Info'] = \"Run Time: \" + str(current_time)\n",
    "df_eHRAF.loc[2, 'run_Info'] = \"Run Date: \" + str(current_date)\n",
    "df_eHRAF.loc[3, 'run_Info'] = \"Run Input: \" + URL_name_nonPlussed\n",
    "df_eHRAF.loc[4, 'run_Info'] = \"Run URL: \" + URL\n",
    "df_eHRAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd()  # get current directory\n",
    "output_dir = \"Data\"  # output directory\n",
    "output_dir_path = directory + '/' + output_dir  # output directory path\n",
    "os.makedirs(output_dir_path, exist_ok=True)  # make Data folder if it does not exist\n",
    "\n",
    "try:\n",
    "    df_eHRAF.to_excel('Data/' + URL_name + '_web_data.xlsx', index=False)\n",
    "except:\n",
    "    print(\"Unable to save the title of the document, please rename it or risk overwriting\")\n",
    "    df_eHRAF.to_excel('Data/' + user + str(now.strftime(\"%m_%d_%y\")) + '_web_data.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3394e5ff5e37174aec0305e0ed7ec30b336f01a56a90646f493ecbcd8deec75b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
